{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "class MarkovChain(object):\n",
    "    def __init__(self, transition_prob):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovChain instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_prob: dict\n",
    "            A dict object representing the transition \n",
    "            probabilities in Markov Chain. \n",
    "            Should be of the form: \n",
    "                {'state1': {'state1': 0.1, 'state2': 0.4}, \n",
    "                 'state2': {...}}\n",
    "        \"\"\"\n",
    "        self.transition_prob = transition_prob\n",
    "        self.states = list(transition_prob.keys())\n",
    " \n",
    "    def next_state(self, current_state):\n",
    "        \"\"\"\n",
    "        Returns the state of the random variable at the next time \n",
    "        instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The current state of the system.\n",
    "        \"\"\"\n",
    "        return np.random.choice(\n",
    "            self.states, \n",
    "            p=[self.transition_prob[current_state][next_state] \n",
    "               for next_state in self.states]\n",
    "        )\n",
    " \n",
    "    def generate_states(self, current_state, no=10):\n",
    "        \"\"\"\n",
    "        Generates the next states of the system.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The state of the current random variable.\n",
    " \n",
    "        no: int\n",
    "            The number of future states to generate.\n",
    "        \"\"\"\n",
    "        future_states = []\n",
    "        for i in range(no):\n",
    "            next_state = self.next_state(current_state)\n",
    "            future_states.append(next_state)\n",
    "            current_state = next_state\n",
    "        return future_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
    "\n",
    "data = df[['v1', 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Iterate through each message, tokenize and normalize it. Each processed token is then added to the \n",
    "word_dict which is a nested dictionary holding all the tokens that follow the given token along with \n",
    "their count corresponding to this token.\n",
    "\n",
    "E.g.,\n",
    "{\n",
    "    \"token_1\": {\"token_2\": 10, \"token_3\": 5}\n",
    "}\n",
    "\n",
    "This data will be used to calculate the probability of each following token given the primary token.\n",
    "The calculated probability is the transition probability from token_1 to token_2 and token_3.\n",
    "\"\"\"\n",
    "def get_word_dictionary(class_tag):\n",
    "    word_dict = {}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        \"\"\"\n",
    "        Fetch the class and the message into two separate variables.\n",
    "        \"\"\"\n",
    "        tag = row['v1']\n",
    "        if tag != class_tag:\n",
    "            continue\n",
    "        \n",
    "        message = row['v2']\n",
    "\n",
    "        \"\"\"\n",
    "        Tokenize the message text and normalize it by removing the punctuations.\n",
    "        \"\"\"\n",
    "        msg_tokens = word_tokenize(message)\n",
    "        last_token = None\n",
    "        for token in msg_tokens:\n",
    "            normalized_token = token.lower()\n",
    "\n",
    "            if normalized_token in string.punctuation:\n",
    "                continue\n",
    "\n",
    "            \"\"\"\n",
    "            Lemmatize the word before adding it to the markov chain.\n",
    "            \"\"\"\n",
    "            lemmatized_token = wordnet_lemmatizer.lemmatize(normalized_token)\n",
    "\n",
    "            if last_token is not None:\n",
    "                if last_token in word_dict:\n",
    "                    sub_dict = word_dict[last_token] \n",
    "                else:\n",
    "                    sub_dict = {}\n",
    "                    word_dict[last_token] = sub_dict\n",
    "\n",
    "                if not lemmatized_token in sub_dict:\n",
    "                    sub_dict[lemmatized_token] = 1 \n",
    "                else:\n",
    "                    sub_dict[lemmatized_token] += 1\n",
    "            else:\n",
    "                word_dict[lemmatized_token] = {}\n",
    "\n",
    "            last_token = lemmatized_token\n",
    "\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word_dict = get_word_dictionary('ham')\n",
    "spam_word_dict = get_word_dictionary('spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
