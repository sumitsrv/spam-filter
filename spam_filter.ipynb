{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "class MarkovChain(object):\n",
    "    def __init__(self, transition_prob):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovChain instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_prob: dict\n",
    "            A dict object representing the transition \n",
    "            probabilities in Markov Chain. \n",
    "            Should be of the form: \n",
    "                {'state1': {'state1': 0.1, 'state2': 0.4}, \n",
    "                 'state2': {...}}\n",
    "        \"\"\"\n",
    "        self.transition_prob = transition_prob\n",
    "        self.states = list(transition_prob.keys())\n",
    " \n",
    "    def next_state(self, current_state):\n",
    "        \"\"\"\n",
    "        Returns the state of the random variable at the next time \n",
    "        instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The current state of the system.\n",
    "        \"\"\"\n",
    "        next_states = list(self.transition_prob[current_state].keys())\n",
    "        return np.random.choice(\n",
    "            next_states, \n",
    "            p=[self.transition_prob[current_state][next_state] \n",
    "               for next_state in next_states]\n",
    "        )\n",
    " \n",
    "    def generate_states(self, current_state, no=10):\n",
    "        \"\"\"\n",
    "        Generates the next states of the system.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The state of the current random variable.\n",
    " \n",
    "        no: int\n",
    "            The number of future states to generate.\n",
    "        \"\"\"\n",
    "        future_states = []\n",
    "        for i in range(no):\n",
    "            next_state = self.next_state(current_state)\n",
    "            future_states.append(next_state)\n",
    "            current_state = next_state\n",
    "        return future_states\n",
    "    \n",
    "    def get_transition_probability_for_sequence(self, current_state, following_states):\n",
    "        \"\"\"\n",
    "        Calculate the transition probability of a sequence of tokens. \n",
    "        Multiply the current probability by the probability of the next transition (current to next state).\n",
    "        The method is called recursively with the first token in the following_states is \n",
    "            deemed to be the next 'current_state'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The current state (token).\n",
    " \n",
    "        following_states: list\n",
    "            The list of the tokens next in sequence.\n",
    "            \n",
    "        prob: float\n",
    "            The probability of the sequence until current_state.\n",
    "        \"\"\"\n",
    "        if len(following_states) <= 1:\n",
    "            return 1\n",
    "        \n",
    "        if current_state not in self.transition_prob:\n",
    "            self.transition_prob[current_state] = {}\n",
    "        \n",
    "        next_state = following_states[0]\n",
    "        \n",
    "        if next_state not in self.transition_prob[current_state]:\n",
    "            self.transition_prob[current_state][next_state] = 1\n",
    "            \n",
    "        prob = self.transition_prob[current_state][next_state] * self.get_transition_probability_for_sequence(next_state, following_states[1:])\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
    "\n",
    "data = df[['v1', 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into train and test set.\n",
    "\"\"\"\n",
    "data_copy = data.copy()\n",
    "train_set = data_copy.sample(frac=0.75, random_state=0)\n",
    "test_set = data_copy.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Iterate through each message, tokenize and normalize it. Each processed token is then added to the \n",
    "word_dict which is a nested dictionary holding all the tokens that follow the given token along with \n",
    "their count corresponding to this token.\n",
    "\n",
    "E.g.,\n",
    "{\n",
    "    \"token_1\": {\"token_2\": 10, \"token_3\": 5}\n",
    "}\n",
    "\n",
    "This data will be used to calculate the probability of each following token given the primary token.\n",
    "The calculated probability is the transition probability from token_1 to token_2 and token_3.\n",
    "\"\"\"\n",
    "def get_word_dictionary(data, class_tag):\n",
    "    word_dict = {}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        \"\"\"\n",
    "        Fetch the class and the message into two separate variables.\n",
    "        \"\"\"\n",
    "        tag = row['v1']\n",
    "        if tag != class_tag:\n",
    "            continue\n",
    "        \n",
    "        message = row['v2']\n",
    "\n",
    "        \"\"\"\n",
    "        Tokenize the message text and normalize it by removing the punctuations.\n",
    "        \"\"\"\n",
    "        msg_tokens = word_tokenize(message)\n",
    "        last_token = None\n",
    "        for token in msg_tokens:\n",
    "            normalized_token = token.lower()\n",
    "\n",
    "            if normalized_token in string.punctuation:\n",
    "                continue\n",
    "\n",
    "            \"\"\"\n",
    "            Lemmatize the word before adding it to the markov chain.\n",
    "            \"\"\"\n",
    "            lemmatized_token = wordnet_lemmatizer.lemmatize(normalized_token)\n",
    "\n",
    "            if last_token is not None:\n",
    "                if last_token in word_dict:\n",
    "                    sub_dict = word_dict[last_token] \n",
    "                else:\n",
    "                    sub_dict = {}\n",
    "                    word_dict[last_token] = sub_dict\n",
    "\n",
    "                if not lemmatized_token in sub_dict:\n",
    "                    sub_dict[lemmatized_token] = 1 \n",
    "                else:\n",
    "                    sub_dict[lemmatized_token] += 1\n",
    "            else:\n",
    "                word_dict[lemmatized_token] = {}\n",
    "\n",
    "            last_token = lemmatized_token\n",
    "\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Iterate through each message, normalize and create the char_dict in an way similar to get_word_dictionary(). In this case\n",
    "spaces need to be considered characters too. Lemmatization is ommitted because it may remove the very information that is \n",
    "unique to the character-sequences compared to word-sequences.\n",
    "E.g.,\n",
    "{\n",
    "    \"char_1\": {\"char_2\": 10, \"char_3\": 9, \"char_4\": 5}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "def get_char_dictionary(data, class_tag):\n",
    "    char_dict = {}\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \"\"\"\n",
    "        Fetch the class and the message into two separate variables.\n",
    "        \"\"\"\n",
    "        tag = row['v1']\n",
    "        if tag != class_tag:\n",
    "            continue\n",
    "        \n",
    "        message = row['v2']\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize the message by turning it lowercase\n",
    "        \"\"\"\n",
    "        normalized_message = message.lower()\n",
    "        last_char = None\n",
    "        \n",
    "        for char in normalized_message:\n",
    "            \"\"\"\n",
    "            If char is part of punctuation don't consider it\n",
    "            \"\"\"\n",
    "            if char in string.punctuation:\n",
    "                continue\n",
    "                \n",
    "            \"\"\"\n",
    "            Generate nested dictionaries and add them to the main one\n",
    "            \"\"\"\n",
    "            if last_char is not None:\n",
    "                if last_char in char_dict:\n",
    "                    sub_dict = char_dict[last_char]\n",
    "                else:\n",
    "                    sub_dict = {}\n",
    "                    char_dict[last_char] = sub_dict\n",
    "                \n",
    "                if not char in sub_dict:\n",
    "                    sub_dict[char] = 1\n",
    "                else:\n",
    "                    sub_dict[char] += 1\n",
    "            else:\n",
    "                char_dict[char] = {}\n",
    "                \n",
    "            last_char = char\n",
    "            \n",
    "    return char_dict\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the word dictionary for the training set of each class type.\n",
    "\"\"\"\n",
    "ham_word_dict_train = get_word_dictionary(train_set, 'ham')\n",
    "spam_word_dict_train = get_word_dictionary(train_set, 'spam')\n",
    "\n",
    "\"\"\"\n",
    "Prepare the word dictionary for the test set of each class type.\n",
    "\"\"\"\n",
    "ham_word_dict_test = get_word_dictionary(test_set, 'ham')\n",
    "spam_word_dict_test = get_word_dictionary(test_set, 'spam')\n",
    "\n",
    "\"\"\"\n",
    "Prepare the char dictionary for the training set of each class type.\n",
    "\"\"\"\n",
    "ham_char_dict_train = get_char_dictionary(train_set, 'ham')\n",
    "spam_char_dict_train = get_char_dictionary(train_set, 'spam')\n",
    "\n",
    "\"\"\"\n",
    "Prepare the char dictionary for the test set of each class type.\n",
    "\"\"\"\n",
    "ham_char_dict_test = get_char_dictionary(test_set, 'ham')\n",
    "spam_char_dict_test = get_char_dictionary(test_set, 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates and returns the transition probabilities for the provided word dictionary. \n",
    "\"\"\"\n",
    "def get_transition_probabilities(dictionary):\n",
    "    prob_dictionary = {}\n",
    "    \n",
    "    for key in dictionary:\n",
    "        sub_dictionary = dictionary[key]\n",
    "        count = 0\n",
    "        for transition in sub_dictionary:\n",
    "            count+=sub_dictionary[transition]\n",
    "        \n",
    "        prob_sub_dictionary = {}\n",
    "        for transition in sub_dictionary:\n",
    "            prob_sub_dictionary[transition] = sub_dictionary[transition]/count\n",
    "        \n",
    "        prob_dictionary[key] = prob_sub_dictionary\n",
    "        \n",
    "    return prob_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the transition probabilities for the training set of all class types for the word chain.\n",
    "\"\"\"\n",
    "ham_word_transition_prob_train = get_transition_probabilities(ham_word_dict_train)\n",
    "spam_word_transition_prob_train = get_transition_probabilities(spam_word_dict_train)\n",
    "\n",
    "\"\"\"\n",
    "Get the transition probabilities for the test set of all class types for the word chain.\n",
    "\"\"\"\n",
    "ham_word_transition_prob_test = get_transition_probabilities(ham_word_dict_test)\n",
    "spam_word_transition_prob_test = get_transition_probabilities(spam_word_dict_test)\n",
    "\n",
    "\"\"\"\n",
    "Get the transition probabilities for the training set of all class types for the character chain.\n",
    "\"\"\"\n",
    "ham_char_transition_prob_train = get_transition_probabilities(ham_char_dict_train)\n",
    "spam_char_transition_prob_train = get_transition_probabilities(spam_char_dict_train)\n",
    "\n",
    "\"\"\"\n",
    "Get the transition probabilities for the test set of all class types for the character chain.\n",
    "\"\"\"\n",
    "ham_char_transition_prob_test = get_transition_probabilities(ham_char_dict_test)\n",
    "spam_char_transition_prob_test = get_transition_probabilities(spam_char_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instantiate four different Markov chain graphs for the Ham and Spam message types for both word and character chains.\n",
    "\"\"\"\n",
    "ham_word_chain = MarkovChain(transition_prob=ham_word_transition_prob_train)\n",
    "spam_word_chain = MarkovChain(transition_prob=spam_word_transition_prob_train)\n",
    "\n",
    "ham_char_chain = MarkovChain(transition_prob=ham_char_transition_prob_train)\n",
    "spam_char_chain = MarkovChain(transition_prob=spam_char_transition_prob_train)\n",
    "\n",
    "# ham_chain.generate_states(current_state='u', no=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham_prob = ham_chain.get_transition_probability_for_sequence()\n",
    "# spam_prob = spam_chain.get_transition_probability_for_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_message_token_sequence(message, token_type):\n",
    "    token_sequence = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalize the message by turning it to lowercase and ignoring punctuations.\n",
    "    \"\"\"\n",
    "    if token_type == 'word':\n",
    "        msg_tokens = word_tokenize(message)\n",
    "    else:\n",
    "        msg_tokens = message.lower()\n",
    "\n",
    "    for token in msg_tokens:\n",
    "        normalized_token = token.lower()\n",
    "\n",
    "        if normalized_token in string.punctuation:\n",
    "            continue\n",
    "        token_sequence.append(normalized_token)\n",
    "        \n",
    "    return token_sequence\n",
    "\n",
    "def process_test_messages(data, token_type):\n",
    "    char_dict = {}\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \"\"\"\n",
    "        Fetch the class and the message into two separate variables.\n",
    "        \"\"\"\n",
    "        tag = row['v1']\n",
    "        \n",
    "        message = row['v2']\n",
    "        if token_type == 'word':\n",
    "            model_tag = process_word_test_message_get_tag(message)\n",
    "        else:\n",
    "            model_tag = process_char_test_message_get_tag(message)\n",
    "            \n",
    "        if model_tag == 0 and tag == 'spam':\n",
    "            TN+=1\n",
    "        if model_tag == 1 and tag == 'spam':\n",
    "            FP+=1\n",
    "        if model_tag == 0 and tag == 'ham':\n",
    "            FN+=1\n",
    "        if model_tag == 1 and tag == 'ham':\n",
    "            TP+=1\n",
    "        \n",
    "    return (TP, FP, TN, FN)\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Processes a raw message, converts it to sequence of tokens and calculates its Ham and Spam probabilities. \n",
    "The method returns 0 if the message is identified as Spam, and 1 if it is a Ham.\n",
    "\"\"\"\n",
    "def process_word_test_message_get_tag(message):\n",
    "    token_sequence = get_test_message_token_sequence(message, 'word')\n",
    "    ham_prob = ham_word_chain.get_transition_probability_for_sequence(token_sequence[0], token_sequence[1:])\n",
    "    spam_prob = spam_word_chain.get_transition_probability_for_sequence(token_sequence[0], token_sequence[1:])\n",
    "    return 1 if ham_prob >= spam_prob else 0\n",
    "\n",
    "\"\"\"\n",
    "Processes a raw message, converts it to sequence of tokens and calculates its Ham and Spam probabilities. \n",
    "The method returns 0 if the message is identified as Spam, and 1 if it is a Ham.\n",
    "\"\"\"\n",
    "def process_char_test_message_get_tag(message):\n",
    "    token_sequence = get_test_message_token_sequence(message, 'char')\n",
    "    ham_prob = ham_char_chain.get_transition_probability_for_sequence(token_sequence[0], token_sequence[1:])\n",
    "    spam_prob = spam_char_chain.get_transition_probability_for_sequence(token_sequence[0], token_sequence[1:])\n",
    "    return 1 if ham_prob >= spam_prob else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Word based processing.\n",
    "\n",
    "TP: Expected - Ham, Predicted - Ham\n",
    "TN: Expected - Spam, Predicted - Spam\n",
    "FP: Expected - Spam, Predicted - Ham\n",
    "FN: Expected - Ham, Predicted - Spam\n",
    "\"\"\"\n",
    "(TP, FP, TN, FN) = process_test_messages(test_set, 'word')\n",
    "\n",
    "precision = TP/(FP + TP)\n",
    "recall = TP / (FN + TP)\n",
    "f1 = 2*precision*recall/ (precision + recall)\n",
    "\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "conf_matrix_word = np.array([[TP, FP], [TN, FN]])\n",
    "ax.matshow(conf_matrix_word, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix_word.shape[0]):\n",
    "    for j in range(conf_matrix_word.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix_word[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Ham', 'Spam'])\n",
    "ax.set_yticklabels(['Ham', 'Spam'])\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Word based Markov chain Spam filtering', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Char based processing.\n",
    "\n",
    "TP: Expected - Ham, Predicted - Ham\n",
    "TN: Expected - Spam, Predicted - Spam\n",
    "FP: Expected - Spam, Predicted - Ham\n",
    "FN: Expected - Ham, Predicted - Spam\n",
    "\"\"\"\n",
    "(TP, FP, TN, FN) = process_test_messages(test_set, 'char')\n",
    "\n",
    "precision = TP/(FP + TP)\n",
    "recall = TP / (FN + TP)\n",
    "f1 = 2*precision*recall/ (precision + recall)\n",
    "\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "conf_matrix_char = np.array([[TP, FP], [TN, FN]])\n",
    "ax.matshow(conf_matrix_char, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix_word.shape[0]):\n",
    "    for j in range(conf_matrix_word.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix_char[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Ham', 'Spam'])\n",
    "ax.set_yticklabels(['Ham', 'Spam'])\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Characters based Markov chain Spam filtering', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
