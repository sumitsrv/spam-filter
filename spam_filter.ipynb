{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "class MarkovChain(object):\n",
    "    def __init__(self, transition_prob):\n",
    "        \"\"\"\n",
    "        Initialize the MarkovChain instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        transition_prob: dict\n",
    "            A dict object representing the transition \n",
    "            probabilities in Markov Chain. \n",
    "            Should be of the form: \n",
    "                {'state1': {'state1': 0.1, 'state2': 0.4}, \n",
    "                 'state2': {...}}\n",
    "        \"\"\"\n",
    "        self.transition_prob = transition_prob\n",
    "        self.states = list(transition_prob.keys())\n",
    " \n",
    "    def next_state(self, current_state):\n",
    "        \"\"\"\n",
    "        Returns the state of the random variable at the next time \n",
    "        instance.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The current state of the system.\n",
    "        \"\"\"\n",
    "        next_states = list(self.transition_prob[current_state].keys())\n",
    "        return np.random.choice(\n",
    "            next_states, \n",
    "            p=[self.transition_prob[current_state][next_state] \n",
    "               for next_state in next_states]\n",
    "        )\n",
    " \n",
    "    def generate_states(self, current_state, no=10):\n",
    "        \"\"\"\n",
    "        Generates the next states of the system.\n",
    " \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The state of the current random variable.\n",
    " \n",
    "        no: int\n",
    "            The number of future states to generate.\n",
    "        \"\"\"\n",
    "        future_states = []\n",
    "        for i in range(no):\n",
    "            next_state = self.next_state(current_state)\n",
    "            future_states.append(next_state)\n",
    "            current_state = next_state\n",
    "        return future_states\n",
    "    \n",
    "    def get_transition_probability_for_sequence(self, current_state, following_states):\n",
    "        \"\"\"\n",
    "        Calculate the transition probability of a sequence of tokens. \n",
    "        Multiply the current probability by the probability of the next transition (current to next state).\n",
    "        The method is called recursively with the first token in the following_states is \n",
    "            deemed to be the next 'current_state'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        current_state: str\n",
    "            The current state (token).\n",
    " \n",
    "        following_states: list\n",
    "            The list of the tokens next in sequence.\n",
    "            \n",
    "        prob: float\n",
    "            The probability of the sequence until current_state.\n",
    "        \"\"\"\n",
    "        if len(following_states) == 1:\n",
    "            return 1\n",
    "        \n",
    "        if self.transition_prob[current_state] is None:\n",
    "            self.transition_prob[current_state] = {}\n",
    "        \n",
    "        next_state = following_states[0]\n",
    "        \n",
    "        if self.transition_prob[current_state][next_state] is None:\n",
    "            self.transition_prob[current_state][next_state] = 1\n",
    "            \n",
    "        prob = self.transition_prob[current_state][next_state] * get_transition_probability_for_sequence(next_state, following_states[1:])\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding='latin-1')\n",
    "\n",
    "data = df[['v1', 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into train and test set.\n",
    "\"\"\"\n",
    "data_copy = data.copy()\n",
    "train_set = data_copy.sample(frac=0.75, random_state=0)\n",
    "test_set = data_copy.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Iterate through each message, tokenize and normalize it. Each processed token is then added to the \n",
    "word_dict which is a nested dictionary holding all the tokens that follow the given token along with \n",
    "their count corresponding to this token.\n",
    "\n",
    "E.g.,\n",
    "{\n",
    "    \"token_1\": {\"token_2\": 10, \"token_3\": 5}\n",
    "}\n",
    "\n",
    "This data will be used to calculate the probability of each following token given the primary token.\n",
    "The calculated probability is the transition probability from token_1 to token_2 and token_3.\n",
    "\"\"\"\n",
    "def get_word_dictionary(data, class_tag):\n",
    "    word_dict = {}\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        \"\"\"\n",
    "        Fetch the class and the message into two separate variables.\n",
    "        \"\"\"\n",
    "        tag = row['v1']\n",
    "        if tag != class_tag:\n",
    "            continue\n",
    "        \n",
    "        message = row['v2']\n",
    "\n",
    "        \"\"\"\n",
    "        Tokenize the message text and normalize it by removing the punctuations.\n",
    "        \"\"\"\n",
    "        msg_tokens = word_tokenize(message)\n",
    "        last_token = None\n",
    "        for token in msg_tokens:\n",
    "            normalized_token = token.lower()\n",
    "\n",
    "            if normalized_token in string.punctuation:\n",
    "                continue\n",
    "\n",
    "            \"\"\"\n",
    "            Lemmatize the word before adding it to the markov chain.\n",
    "            \"\"\"\n",
    "            lemmatized_token = wordnet_lemmatizer.lemmatize(normalized_token)\n",
    "\n",
    "            if last_token is not None:\n",
    "                if last_token in word_dict:\n",
    "                    sub_dict = word_dict[last_token] \n",
    "                else:\n",
    "                    sub_dict = {}\n",
    "                    word_dict[last_token] = sub_dict\n",
    "\n",
    "                if not lemmatized_token in sub_dict:\n",
    "                    sub_dict[lemmatized_token] = 1 \n",
    "                else:\n",
    "                    sub_dict[lemmatized_token] += 1\n",
    "            else:\n",
    "                word_dict[lemmatized_token] = {}\n",
    "\n",
    "            last_token = lemmatized_token\n",
    "\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Iterate through each message, normalize and create the char_dict in an way similar to get_word_dictionary(). In this case\n",
    "spaces need to be considered characters too. Lemmatization is ommitted because it may remove the very information that is \n",
    "unique to the character-sequences compared to word-sequences.\n",
    "E.g.,\n",
    "{\n",
    "    \"char_1\": {\"char_2\": 10, \"char_3\": 9, \"char_4\": 5}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "def get_char_dictionary(data, class_tag):\n",
    "    char_dict = {}\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        \"\"\"\n",
    "        Fetch the class and the message into two separate variables.\n",
    "        \"\"\"\n",
    "        tag = row['v1']\n",
    "        if tag != class_tag:\n",
    "            continue\n",
    "        \n",
    "        message = row['v2']\n",
    "\n",
    "        \"\"\"\n",
    "        Normalize the message by turning it lowercase\n",
    "        \"\"\"\n",
    "        normalized_message = message.lower()\n",
    "        last_char = None\n",
    "        \n",
    "        for char in normalized_message:\n",
    "            \"\"\"\n",
    "            If char is part of punctuation don't consider it\n",
    "            \"\"\"\n",
    "            if char in string.punctuation:\n",
    "                continue\n",
    "                \n",
    "            \"\"\"\n",
    "            Generate nested dictionaries and add them to the main one\n",
    "            \"\"\"\n",
    "            if last_char is not None:\n",
    "                if last_char in char_dict:\n",
    "                    sub_dict = char_dict[last_char]\n",
    "                else:\n",
    "                    sub_dict = {}\n",
    "                    char_dict[last_char] = sub_dict\n",
    "                \n",
    "                if not char in sub_dict:\n",
    "                    sub_dict[char] = 1\n",
    "                else:\n",
    "                    sub_dict[char] += 1\n",
    "            else:\n",
    "                char_dict[char] = {}\n",
    "                \n",
    "            last_char = char\n",
    "            \n",
    "    return char_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare the word dictionary for the training set of each class type.\n",
    "\"\"\"\n",
    "ham_word_dict_train = get_word_dictionary(train_set, 'ham')\n",
    "spam_word_dict_train = get_word_dictionary(train_set, 'spam')\n",
    "\n",
    "\"\"\"\n",
    "Prepare the word dictionary for the test set of each class type.\n",
    "\"\"\"\n",
    "ham_word_dict_test = get_word_dictionary(test_set, 'ham')\n",
    "spam_word_dict_test = get_word_dictionary(test_set, 'spam')\n",
    "\n",
    "\"\"\"\n",
    "Prepare the char dictionary for the training set of each class type.\n",
    "\"\"\"\n",
    "ham_char_dict_train = get_char_dictionary(train_set, 'ham')\n",
    "spam_char_dict_train = get_char_dictionary(train_set, 'spam')\n",
    "\n",
    "\"\"\"\n",
    "Prepare the char dictionary for the test set of each class type.\n",
    "\"\"\"\n",
    "ham_char_dict_test = get_char_dictionary(test_set, 'ham')\n",
    "spam_char_dict_test = get_char_dictionary(test_set, 'spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates and returns the transition probabilities for the provided word dictionary. \n",
    "\"\"\"\n",
    "def get_transition_probabilities(dictionary):\n",
    "    prob_dictionary = {}\n",
    "    \n",
    "    for key in dictionary:\n",
    "        sub_dictionary = dictionary[key]\n",
    "        count = 0\n",
    "        for transition in sub_dictionary:\n",
    "            count+=sub_dictionary[transition]\n",
    "        \n",
    "        prob_sub_dictionary = {}\n",
    "        for transition in sub_dictionary:\n",
    "            prob_sub_dictionary[transition] = sub_dictionary[transition]/count\n",
    "        \n",
    "        prob_dictionary[key] = prob_sub_dictionary\n",
    "        \n",
    "    return prob_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the transition probabilities for the training set of all class types for the word chain.\n",
    "\"\"\"\n",
    "ham_word_transition_prob_train = get_transition_probabilities(ham_word_dict_train)\n",
    "spam_word_transition_prob_train = get_transition_probabilities(spam_word_dict_train)\n",
    "\n",
    "\"\"\"\n",
    "Get the transition probabilities for the test set of all class types for the word chain.\n",
    "\"\"\"\n",
    "ham_word_transition_prob_test = get_transition_probabilities(ham_word_dict_test)\n",
    "spam_word_transition_prob_test = get_transition_probabilities(spam_word_dict_test)\n",
    "\n",
    "\"\"\"\n",
    "Get the transition probabilities for the training set of all class types for the character chain.\n",
    "\"\"\"\n",
    "ham_char_transition_prob_train = get_transition_probabilities(ham_char_dict_train)\n",
    "spam_char_transition_prob_train = get_transition_probabilities(spam_char_dict_train)\n",
    "\n",
    "\"\"\"\n",
    "Get the transition probabilities for the test set of all class types for the character chain.\n",
    "\"\"\"\n",
    "ham_char_transition_prob_test = get_transition_probabilities(ham_char_dict_test)\n",
    "spam_char_transition_prob_test = get_transition_probabilities(spam_char_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instantiate four different Markov chain graphs for the Ham and Spam message types for both word and character chains.\n",
    "\"\"\"\n",
    "ham_word_chain = MarkovChain(transition_prob=ham_word_transition_prob_train)\n",
    "spam_word_chain = MarkovChain(transition_prob=spam_word_transition_prob_train)\n",
    "\n",
    "ham_char_chain = MarkovChain(transition_prob=ham_char_transition_prob_train)\n",
    "spam_char_chain = MarkovChain(transition_prob=spam_char_transition_prob_train)\n",
    "\n",
    "# ham_chain.generate_states(current_state='u', no=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham_prob = ham_chain.get_transition_probability_for_sequence()\n",
    "# spam_prob = spam_chain.get_transition_probability_for_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
